{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import spacy\n",
    "import numpy as np\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatTextDataset(torchtext.data.Dataset):\n",
    "    \"\"\"\n",
    "    Form torchtext dataset from all files in given path.\n",
    "    This is needed to be constructed with .splits() method. \n",
    "    Not with this __init__ constructor. \n",
    "    \n",
    "    .splits() actually will call this multiple times for different directories \n",
    "    given by train, test and validation parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, path, text_field, newline_eos=True, encoding='utf-8', **kwargs):\n",
    "        fields = [('text', text_field)]\n",
    "        text = []\n",
    "\n",
    "        if os.path.isdir(path): \n",
    "            paths=glob.glob(f'{path}/*.*')\n",
    "        else: \n",
    "            paths=[path]\n",
    "        \n",
    "        for p in paths:\n",
    "            for line in open(p, encoding=encoding): \n",
    "                text += text_field.preprocess(line)\n",
    "            if newline_eos: \n",
    "                text.append('<eos>')\n",
    "\n",
    "        examples = [torchtext.data.Example.fromlist([text], fields)]\n",
    "        super().__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tok = spacy.load('en')\n",
    "text = torchtext.data.Field(lower=True, tokenize='spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you dont have `data/lm_*.data` for some reason you could run two cells bellow this, to download IMDB datasest and process it. **WARNING** it takes long time to run. Else just load saved data and create torhctext datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P data/ https://files.fast.ai/data/aclImdb.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING takes long time to run\n",
    "train_ds, valid_ds, test_ds  = ConcatTextDataset.splits('data/aclImdb/',\n",
    "                                                        text_field=text, \n",
    "                                                        train='train/all/',\n",
    "                                                        validation='test/all/', \n",
    "                                                        test='test/all/')\n",
    "\n",
    "if not Path('dat/models/text.pkl').exists():\n",
    "    with open('data/models/text.pkl', 'wb') as f:\n",
    "        dill.dump(text, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preprocessed data for language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path('data/models/text.pkl').exists():\n",
    "    with open('data/models/text.pkl', 'rb') as f:\n",
    "        text = dill.load(f)\n",
    "\n",
    "train_ex = torch.load('data/lm_train.data')\n",
    "valid_ex = torch.load('data/lm_valid.data')\n",
    "\n",
    "train_ds = torchtext.data.Dataset([train_ex], [('text', text)])\n",
    "valid_ds = torchtext.data.Dataset([valid_ex], [('text', text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "    \"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements dropout with same mask for each time step\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x, p=0.5):\n",
    "        \"\"\"\n",
    "        Forward step. x has following dimensions: \n",
    "            (time, samples, input_dim)\n",
    "        \"\"\"\n",
    "        if not p or not self.training:\n",
    "            return x\n",
    "        \n",
    "        mask = torch.empty(1, x.size(1), x.size(2)).bernoulli_(1 - p) / (1 - p)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithDropout(nn.Embedding):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, inp, p=0.5):\n",
    "        if p and self.training:\n",
    "            size = (self.weight.size(0), 1)\n",
    "            mask = torch.empty(size).bernoulli_(1 - p) / (1 - p)\n",
    "            mask = mask.expand_as(self.weight)\n",
    "            dropout_weight = mask * self.weight\n",
    "        else:\n",
    "            dropout_weight = self.weight\n",
    "\n",
    "        padding_idx = self.padding_idx\n",
    "        if padding_idx is None:\n",
    "            padding_idx = -1\n",
    "        \n",
    "        x = torch.nn.functional.embedding(inp, dropout_weight, padding_idx, \n",
    "                                          self.max_norm, self.norm_type,\n",
    "                                          self.scale_grad_by_freq, self.sparse)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN for language modeling with input embedding n_layers rnn layers\n",
    "    and linear decoder\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, n_layers, pad_token,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, dropouto=0.5):\n",
    "        \"\"\"\n",
    "        Dropout probabilities:\n",
    "        * dropouth -- hidden to hidden\n",
    "        * dropouti -- input dropout\n",
    "        * dropoute -- embedding dropout\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropoute = dropoute\n",
    "        self.dropouth = dropouth\n",
    "        self.dropouti = dropouti\n",
    "        self.dropouto = dropouto\n",
    "        self.lockdrop = RecurrentDropout()\n",
    "\n",
    "        self.word_embeddings = EmbeddingWithDropout(vocab_size, embedding_dim, padding_idx=pad_token)\n",
    "        self.rnns = [nn.LSTM(embedding_dim if l == 0 else hidden_dim, hidden_dim, num_layers=1) \n",
    "                     for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList(self.rnns)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.word_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        emb = self.word_embeddings(inp)\n",
    "        emb = self.lockdrop(emb, p=self.dropouti)\n",
    "        cur_inp = emb\n",
    "        raw_outputs, outputs, new_hidden = [], [], []\n",
    "        \n",
    "        for l, rnn in enumerate(self.rnns):\n",
    "            output, new_h = rnn(cur_inp, hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(output)\n",
    "            \n",
    "            if l != self.n_layers - 1:\n",
    "                output = self.lockdrop(output, p=self.dropouth)\n",
    "                outputs.append(output)\n",
    "\n",
    "            cur_inp = output\n",
    "\n",
    "        output = self.lockdrop(output, p=self.dropouto)\n",
    "        decoded = self.decoder(output.view(-1, output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return [(weight.new(1, batch_size, self.hidden_dim).zero_(),\n",
    "                 weight.new(1, batch_size, self.hidden_dim).zero_())\n",
    "                    for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, valid_it):\n",
    "    model.eval()\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, criterion, optimizer, train_it, vocab_size, valid_it=None, clip_grad=.25):\n",
    "    \"\"\"\n",
    "    Train model for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "    avg_mom = .98\n",
    "    start_time = time.time()\n",
    "    \n",
    "    hidden = model.init_hidden(train_it.batch_size)\n",
    "    pbar = tqdm(train_it, total=len(train_it), leave=False, ascii=True)\n",
    "    \n",
    "    for i, batch in enumerate(pbar):\n",
    "        text, target = batch.text, batch.target\n",
    "        # detach history from tensor\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        # run model on batch of data\n",
    "        model.zero_grad()\n",
    "        prediction, hidden = model(text, hidden)\n",
    "        loss = criterion(prediction.view(-1, vocab_size), target.view(-1))\n",
    "\n",
    "        # output average loss to progress bar\n",
    "        avg_loss = avg_loss * avg_mom + loss.item() * (1 - avg_mom)\n",
    "        debias_loss = avg_loss / (1 - avg_mom**(i+1))\n",
    "        pbar.set_postfix(loss=debias_loss, refresh=False)\n",
    "\n",
    "        # optimizer step\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text.vocab)\n",
    "model = RNNmodel(300, 128, vocab_size, \n",
    "                 n_layers=1, \n",
    "                 pad_token=text.vocab.stoi[text.pad_token])\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3, betas=(0.7, 0.99))\n",
    "\n",
    "train_it = torchtext.data.BPTTIterator(train_ds, batch_size=64, bptt_len=70, device=-1)\n",
    "valid_it = torchtext.data.BPTTIterator(valid_ds, batch_size=64, bptt_len=70, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(1, model, criterion, optimizer, train_it, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
